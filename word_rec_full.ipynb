{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_rec_full.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIR0ytSuPccy"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QhRuGZdDhnL",
        "outputId": "e6fd36c5-1b61-434e-eb38-9629bf936416"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6_rZNq1DzQ2",
        "outputId": "c23f097a-e3ff-4971-dfae-e844137f3e83"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/simpleHTRLine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/simpleHTRLine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoPjUkVJEAc-",
        "outputId": "0f07996a-1675-44c8-b230-ee2fb30b7838"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mCTCWordBeamSearch\u001b[0m/  \u001b[01;34mdump\u001b[0m/   \u001b[01;34msrc\u001b[0m/                 wordRec.ipynb\n",
            "\u001b[01;34mdata\u001b[0m/               \u001b[01;34mmodel\u001b[0m/  word_rec_full.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C0fnSJbPieB"
      },
      "source": [
        "#Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "BJarAFJ1ECuq",
        "outputId": "951d9eb4-18c0-4cd4-a266-451233374c8e"
      },
      "source": [
        "%pip install path\n",
        "%pip install typing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting path\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/d6/68e26fb6204e280c770deffcd3062a380b769436740196a26fbcc7f6e2d4/path-16.0.0-py3-none-any.whl\n",
            "Installing collected packages: path\n",
            "Successfully installed path-16.0.0\n",
            "Collecting typing\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-cp37-none-any.whl size=26323 sha256=1217f1fccc2d0407d0c501d823f9f593b4eda741c9ae7bd6cb264d248603cb51\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "Successfully built typing\n",
            "Installing collected packages: typing\n",
            "Successfully installed typing-3.7.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q9mqOrwEblf"
      },
      "source": [
        "from collections import namedtuple\n",
        "from typing import Tuple, List\n",
        "import json\n",
        "import cv2\n",
        "import editdistance\n",
        "from path import Path\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6cj1IDwPuMZ"
      },
      "source": [
        "#DataLoader Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNS0ZjgKG1qx"
      },
      "source": [
        "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
        "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')\n",
        "\n",
        "\n",
        "class DataLoaderIAM:\n",
        "    \"\"\"\n",
        "    Loads data which corresponds to IAM format,\n",
        "    see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir: Path,\n",
        "                 batch_size: int,\n",
        "                 data_split: float = 0.95,\n",
        "                 fast: bool = False) -> None:\n",
        "        \"\"\"Loader for dataset.\"\"\"\n",
        "\n",
        "        #assert data_dir.exists()\n",
        "\n",
        "        self.fast = fast\n",
        "        if fast:\n",
        "            self.env = lmdb.open(str(data_dir / 'lmdb'), readonly=True)\n",
        "\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = []\n",
        "\n",
        "        f = open(data_dir + 'gt/words.txt')\n",
        "        chars = set()\n",
        "        bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']  # known broken images in IAM dataset\n",
        "        for line in f:\n",
        "            # ignore comment line\n",
        "            if not line or line[0] == '#':\n",
        "                continue\n",
        "\n",
        "            line_split = line.strip().split(' ')\n",
        "            assert len(line_split) >= 9\n",
        "\n",
        "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
        "            file_name_split = line_split[0].split('-')\n",
        "            file_name_subdir1 = file_name_split[0]\n",
        "            file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
        "            file_base_name = line_split[0] + '.png'\n",
        "            file_name = data_dir + 'img' + file_name_subdir1 + file_name_subdir2 + file_base_name\n",
        "\n",
        "            if line_split[0] in bad_samples_reference:\n",
        "                print('Ignoring known broken image:', file_name)\n",
        "                continue\n",
        "\n",
        "            # GT text are columns starting at 9\n",
        "            gt_text = ' '.join(line_split[8:])\n",
        "            chars = chars.union(set(list(gt_text)))\n",
        "\n",
        "            # put sample into list\n",
        "            self.samples.append(Sample(gt_text, file_name))\n",
        "\n",
        "        # split into training and validation set: 95% - 5%\n",
        "        split_idx = int(data_split * len(self.samples))\n",
        "        self.train_samples = self.samples[:split_idx]\n",
        "        self.validation_samples = self.samples[split_idx:]\n",
        "\n",
        "        # put words into lists\n",
        "        self.train_words = [x.gt_text for x in self.train_samples]\n",
        "        self.validation_words = [x.gt_text for x in self.validation_samples]\n",
        "\n",
        "        # start with train set\n",
        "        self.train_set()\n",
        "\n",
        "        # list of all chars in dataset\n",
        "        self.char_list = sorted(list(chars))\n",
        "\n",
        "    def train_set(self) -> None:\n",
        "        \"\"\"Switch to randomly chosen subset of training set.\"\"\"\n",
        "        self.data_augmentation = True\n",
        "        self.curr_idx = 0\n",
        "        random.shuffle(self.train_samples)\n",
        "        self.samples = self.train_samples\n",
        "        self.curr_set = 'train'\n",
        "\n",
        "    def validation_set(self) -> None:\n",
        "        \"\"\"Switch to validation set.\"\"\"\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.samples = self.validation_samples\n",
        "        self.curr_set = 'val'\n",
        "\n",
        "    def get_iterator_info(self) -> Tuple[int, int]:\n",
        "        \"\"\"Current batch index and overall number of batches.\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            num_batches = int(np.floor(len(self.samples) / self.batch_size))  # train set: only full-sized batches\n",
        "        else:\n",
        "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))  # val set: allow last batch to be smaller\n",
        "        curr_batch = self.curr_idx // self.batch_size + 1\n",
        "        return curr_batch, num_batches\n",
        "\n",
        "    def has_next(self) -> bool:\n",
        "        \"\"\"Is there a next element?\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            return self.curr_idx + self.batch_size <= len(self.samples)  # train set: only full-sized batches\n",
        "        else:\n",
        "            return self.curr_idx < len(self.samples)  # val set: allow last batch to be smaller\n",
        "\n",
        "    def _get_img(self, i: int) -> np.ndarray:\n",
        "        if self.fast:\n",
        "            with self.env.begin() as txn:\n",
        "                basename = Path(self.samples[i].file_path).basename()\n",
        "                data = txn.get(basename.encode(\"ascii\"))\n",
        "                img = pickle.loads(data)\n",
        "        else:\n",
        "            img = cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def get_next(self) -> Batch:\n",
        "        \"\"\"Get next element.\"\"\"\n",
        "        batch_range = range(self.curr_idx, min(self.curr_idx + self.batch_size, len(self.samples)))\n",
        "\n",
        "        imgs = [self._get_img(i) for i in batch_range]\n",
        "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
        "\n",
        "        self.curr_idx += self.batch_size\n",
        "        return Batch(imgs, gt_texts, len(imgs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZrtjljLP1Zf"
      },
      "source": [
        "#Preprocessor Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0Y5CqApFg_1"
      },
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self,\n",
        "                 img_size: Tuple[int, int],\n",
        "                 padding: int = 0,\n",
        "                 dynamic_width: bool = False,\n",
        "                 data_augmentation: bool = False,\n",
        "                 line_mode: bool = False) -> None:\n",
        "        # dynamic width only supported when no data augmentation happens\n",
        "        assert not (dynamic_width and data_augmentation)\n",
        "        # when padding is on, we need dynamic width enabled\n",
        "        assert not (padding > 0 and not dynamic_width)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.padding = padding\n",
        "        self.dynamic_width = dynamic_width\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.line_mode = line_mode\n",
        "\n",
        "    @staticmethod\n",
        "    def _truncate_label(text: str, max_text_len: int) -> str:\n",
        "        \"\"\"\n",
        "        Function ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
        "        labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
        "        If a too-long label is provided, ctc_loss returns an infinite gradient.\n",
        "        \"\"\"\n",
        "        cost = 0\n",
        "        for i in range(len(text)):\n",
        "            if i != 0 and text[i] == text[i - 1]:\n",
        "                cost += 2\n",
        "            else:\n",
        "                cost += 1\n",
        "            if cost > max_text_len:\n",
        "                return text[:i]\n",
        "        return text\n",
        "\n",
        "    def _simulate_text_line(self, batch: Batch) -> Batch:\n",
        "        \"\"\"Create image of a text line by pasting multiple word images into an image.\"\"\"\n",
        "\n",
        "        default_word_sep = 30\n",
        "        default_num_words = 5\n",
        "\n",
        "        # go over all batch elements\n",
        "        res_imgs = []\n",
        "        res_gt_texts = []\n",
        "        for i in range(batch.batch_size):\n",
        "            # number of words to put into current line\n",
        "            num_words = random.randint(1, 8) if self.data_augmentation else default_num_words\n",
        "\n",
        "            # concat ground truth texts\n",
        "            curr_gt = ' '.join([batch.gt_texts[(i + j) % batch.batch_size] for j in range(num_words)])\n",
        "            res_gt_texts.append(curr_gt)\n",
        "\n",
        "            # put selected word images into list, compute target image size\n",
        "            sel_imgs = []\n",
        "            word_seps = [0]\n",
        "            h = 0\n",
        "            w = 0\n",
        "            for j in range(num_words):\n",
        "                curr_sel_img = batch.imgs[(i + j) % batch.batch_size]\n",
        "                curr_word_sep = random.randint(20, 50) if self.data_augmentation else default_word_sep\n",
        "                h = max(h, curr_sel_img.shape[0])\n",
        "                w += curr_sel_img.shape[1]\n",
        "                sel_imgs.append(curr_sel_img)\n",
        "                if j + 1 < num_words:\n",
        "                    w += curr_word_sep\n",
        "                    word_seps.append(curr_word_sep)\n",
        "\n",
        "            # put all selected word images into target image\n",
        "            target = np.ones([h, w], np.uint8) * 255\n",
        "            x = 0\n",
        "            for curr_sel_img, curr_word_sep in zip(sel_imgs, word_seps):\n",
        "                x += curr_word_sep\n",
        "                y = (h - curr_sel_img.shape[0]) // 2\n",
        "                target[y:y + curr_sel_img.shape[0]:, x:x + curr_sel_img.shape[1]] = curr_sel_img\n",
        "                x += curr_sel_img.shape[1]\n",
        "\n",
        "            # put image of line into result\n",
        "            res_imgs.append(target)\n",
        "\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
        "\n",
        "    def process_img(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Resize to target size, apply data augmentation.\"\"\"\n",
        "\n",
        "        # there are damaged files in IAM dataset - just use black image instead\n",
        "        if img is None:\n",
        "            img = np.zeros(self.img_size[::-1])\n",
        "\n",
        "        # data augmentation\n",
        "        img = img.astype(np.float)\n",
        "        if self.data_augmentation:\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.25:\n",
        "                def rand_odd():\n",
        "                    return random.randint(1, 3) * 2 + 1\n",
        "                img = cv2.GaussianBlur(img, (rand_odd(), rand_odd()), 0)\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.dilate(img, np.ones((3, 3)))\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.erode(img, np.ones((3, 3)))\n",
        "\n",
        "            # geometric data augmentation\n",
        "            wt, ht = self.img_size\n",
        "            h, w = img.shape\n",
        "            f = min(wt / w, ht / h)\n",
        "            fx = f * np.random.uniform(0.75, 1.05)\n",
        "            fy = f * np.random.uniform(0.75, 1.05)\n",
        "\n",
        "            # random position around center\n",
        "            txc = (wt - w * fx) / 2\n",
        "            tyc = (ht - h * fy) / 2\n",
        "            freedom_x = max((wt - fx * w) / 2, 0)\n",
        "            freedom_y = max((ht - fy * h) / 2, 0)\n",
        "            tx = txc + np.random.uniform(-freedom_x, freedom_x)\n",
        "            ty = tyc + np.random.uniform(-freedom_y, freedom_y)\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[fx, 0, tx], [0, fy, ty]])\n",
        "            target = np.ones(self.img_size[::-1]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=self.img_size, dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.5:\n",
        "                img = img * (0.25 + random.random() * 0.75)\n",
        "            if random.random() < 0.25:\n",
        "                img = np.clip(img + (np.random.random(img.shape) - 0.5) * random.randint(1, 25), 0, 255)\n",
        "            if random.random() < 0.1:\n",
        "                img = 255 - img\n",
        "\n",
        "        # no data augmentation\n",
        "        else:\n",
        "            if self.dynamic_width:\n",
        "                ht = self.img_size[1]\n",
        "                h, w = img.shape\n",
        "                f = ht / h\n",
        "                wt = int(f * w + self.padding)\n",
        "                wt = wt + (4 - wt) % 4\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = 0\n",
        "            else:\n",
        "                wt, ht = self.img_size\n",
        "                h, w = img.shape\n",
        "                f = min(wt / w, ht / h)\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = (ht - h * f) / 2\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[f, 0, tx], [0, f, ty]])\n",
        "            target = np.ones([ht, wt]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=(wt, ht), dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "        # transpose for TF\n",
        "        img = cv2.transpose(img)\n",
        "\n",
        "        # convert to range [-1, 1]\n",
        "        img = img / 255 - 0.5\n",
        "        return img\n",
        "\n",
        "    def process_batch(self, batch: Batch) -> Batch:\n",
        "        if self.line_mode:\n",
        "            batch = self._simulate_text_line(batch)\n",
        "\n",
        "        res_imgs = [self.process_img(img) for img in batch.imgs]\n",
        "        max_text_len = res_imgs[0].shape[0] // 4\n",
        "        res_gt_texts = [self._truncate_label(gt_text, max_text_len) for gt_text in batch.gt_texts]\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUuv2N-YQBzf"
      },
      "source": [
        "#Tetsing Preprocessor Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN3D8YvUGh9X"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def test_preprocess(img_path):\n",
        "  img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "  img_aug = Preprocessor((256, 32), data_augmentation=True).process_img(img)\n",
        "  plt.subplot(121)\n",
        "  plt.imshow(img, cmap='gray')\n",
        "  plt.subplot(122)\n",
        "  plt.imshow(cv2.transpose(img_aug) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "WnWlNxOx-fnd",
        "outputId": "faea4475-f2eb-4cec-f735-89f807369efa"
      },
      "source": [
        "test_preprocess('/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/data/t16.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tBOdAj9QKSW"
      },
      "source": [
        "#Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfrH8vOSHL8x"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class DecoderType:\n",
        "    \"\"\"CTC decoder types.\"\"\"\n",
        "    BestPath = 0\n",
        "    BeamSearch = 1\n",
        "    WordBeamSearch = 2\n",
        "\n",
        "\n",
        "class Model:\n",
        "    \"\"\"Minimalistic TF model for HTR.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 char_list: List[str],\n",
        "                 decoder_type: str = DecoderType.BestPath,\n",
        "                 must_restore: bool = False,\n",
        "                 dump: bool = False) -> None:\n",
        "        \"\"\"Init model: add CNN, RNN and CTC and initialize TF.\"\"\"\n",
        "        self.dump = dump\n",
        "        self.char_list = char_list\n",
        "        self.decoder_type = decoder_type\n",
        "        self.must_restore = must_restore\n",
        "        self.snap_ID = 0\n",
        "\n",
        "        # Whether to use normalization over a batch or a population\n",
        "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
        "\n",
        "        # input image batch\n",
        "        self.input_imgs = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))\n",
        "\n",
        "        # setup CNN, RNN and CTC\n",
        "        self.setup_cnn()\n",
        "        self.setup_rnn()\n",
        "        self.setup_ctc()\n",
        "\n",
        "        # setup optimizer to train NN\n",
        "        self.batches_trained = 0\n",
        "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(self.update_ops):\n",
        "            self.optimizer = tf.compat.v1.train.AdamOptimizer().minimize(self.loss)\n",
        "\n",
        "        # initialize TF\n",
        "        self.sess, self.saver = self.setup_tf()\n",
        "\n",
        "    def setup_cnn(self) -> None:\n",
        "        \"\"\"Create CNN layers.\"\"\"\n",
        "        cnn_in4d = tf.expand_dims(input=self.input_imgs, axis=3)\n",
        "\n",
        "        # list of parameters for the layers\n",
        "        kernel_vals = [5, 5, 3, 3, 3]\n",
        "        feature_vals = [1, 32, 64, 128, 128, 256]\n",
        "        stride_vals = pool_vals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
        "        num_layers = len(stride_vals)\n",
        "\n",
        "        # create layers\n",
        "        pool = cnn_in4d  # input to first CNN layer\n",
        "        for i in range(num_layers):\n",
        "            kernel = tf.Variable(\n",
        "                tf.random.truncated_normal([kernel_vals[i], kernel_vals[i], feature_vals[i], feature_vals[i + 1]],\n",
        "                                           stddev=0.1))\n",
        "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
        "            relu = tf.nn.relu(conv_norm)\n",
        "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, pool_vals[i][0], pool_vals[i][1], 1),\n",
        "                                    strides=(1, stride_vals[i][0], stride_vals[i][1], 1), padding='VALID')\n",
        "\n",
        "        self.cnn_out_4d = pool\n",
        "\n",
        "    def setup_rnn(self) -> None:\n",
        "        \"\"\"Create RNN layers.\"\"\"\n",
        "        rnn_in3d = tf.squeeze(self.cnn_out_4d, axis=[2])\n",
        "\n",
        "        # basic cells which is used to build RNN\n",
        "        num_hidden = 256\n",
        "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n",
        "                 range(2)]  # 2 layers\n",
        "\n",
        "        # stack basic cells\n",
        "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "        # bidirectional RNN\n",
        "        # BxTxF -> BxTx2H\n",
        "        (fw, bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_in3d,\n",
        "                                                                dtype=rnn_in3d.dtype)\n",
        "\n",
        "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
        "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
        "\n",
        "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
        "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, num_hidden * 2, len(self.char_list) + 1], stddev=0.1))\n",
        "        self.rnn_out_3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'),\n",
        "                                     axis=[2])\n",
        "\n",
        "    def setup_ctc(self) -> None:\n",
        "        \"\"\"Create CTC loss and decoder.\"\"\"\n",
        "        # BxTxC -> TxBxC\n",
        "        self.ctc_in_3d_tbc = tf.transpose(a=self.rnn_out_3d, perm=[1, 0, 2])\n",
        "        # ground truth text as sparse tensor\n",
        "        self.gt_texts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
        "                                        tf.compat.v1.placeholder(tf.int32, [None]),\n",
        "                                        tf.compat.v1.placeholder(tf.int64, [2]))\n",
        "\n",
        "        # calc loss for batch\n",
        "        self.seq_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "        self.loss = tf.reduce_mean(\n",
        "            input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.ctc_in_3d_tbc,\n",
        "                                                  sequence_length=self.seq_len,\n",
        "                                                  ctc_merge_repeated=True))\n",
        "\n",
        "        # calc loss for each element to compute label probability\n",
        "        self.saved_ctc_input = tf.compat.v1.placeholder(tf.float32,\n",
        "                                                        shape=[None, None, len(self.char_list) + 1])\n",
        "        self.loss_per_element = tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.saved_ctc_input,\n",
        "                                                         sequence_length=self.seq_len, ctc_merge_repeated=True)\n",
        "\n",
        "        # best path decoding or beam search decoding\n",
        "        if self.decoder_type == DecoderType.BestPath:\n",
        "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len)\n",
        "        elif self.decoder_type == DecoderType.BeamSearch:\n",
        "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len,\n",
        "                                                         beam_width=50)\n",
        "        # word beam search decoding (see https://github.com/githubharald/CTCWordBeamSearch)\n",
        "        elif self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
        "            chars = ''.join(self.char_list)\n",
        "            word_chars = open('/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/wordCharList.txt').read().splitlines()[0]\n",
        "            corpus = open('/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/data/corpus.txt').read()\n",
        "\n",
        "            # decode using the \"Words\" mode of word beam search\n",
        "            from word_beam_search import WordBeamSearch\n",
        "            self.decoder = WordBeamSearch(50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'),\n",
        "                                          word_chars.encode('utf8'))\n",
        "\n",
        "            # the input to the decoder must have softmax already applied\n",
        "            self.wbs_input = tf.nn.softmax(self.ctc_in_3d_tbc, axis=2)\n",
        "\n",
        "    def setup_tf(self) -> Tuple[tf.compat.v1.Session, tf.compat.v1.train.Saver]:\n",
        "        \"\"\"Initialize TF.\"\"\"\n",
        "        #print('Python: ' + sys.version)\n",
        "        #print('Tensorflow: ' + tf.__version__)\n",
        "\n",
        "        sess = tf.compat.v1.Session()  # TF session\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
        "        model_dir = '/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/'\n",
        "        latest_snapshot = tf.train.latest_checkpoint(model_dir)  # is there a saved model?\n",
        "\n",
        "        # if model must be restored (for inference), there must be a snapshot\n",
        "        if self.must_restore and not latest_snapshot:\n",
        "            raise Exception('No saved model found in: ' + model_dir)\n",
        "\n",
        "        # load saved model if available\n",
        "        if latest_snapshot:\n",
        "            print('Init with stored values from ' + latest_snapshot)\n",
        "            saver.restore(sess, latest_snapshot)\n",
        "        else:\n",
        "            print('Init with new values')\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        return sess, saver\n",
        "\n",
        "    def to_sparse(self, texts: List[str]) -> Tuple[List[List[int]], List[int], List[int]]:\n",
        "        \"\"\"Put ground truth texts into sparse tensor for ctc_loss.\"\"\"\n",
        "        indices = []\n",
        "        values = []\n",
        "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
        "\n",
        "        # go over all texts\n",
        "        for batchElement, text in enumerate(texts):\n",
        "            # convert to string of label (i.e. class-ids)\n",
        "            label_str = [self.char_list.index(c) for c in text]\n",
        "            # sparse tensor must have size of max. label-string\n",
        "            if len(label_str) > shape[1]:\n",
        "                shape[1] = len(label_str)\n",
        "            # put each label into sparse tensor\n",
        "            for i, label in enumerate(label_str):\n",
        "                indices.append([batchElement, i])\n",
        "                values.append(label)\n",
        "\n",
        "        return indices, values, shape\n",
        "\n",
        "    def decoder_output_to_text(self, ctc_output: tuple, batch_size: int) -> List[str]:\n",
        "        \"\"\"Extract texts from output of CTC decoder.\"\"\"\n",
        "\n",
        "        # word beam search: already contains label strings\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            label_strs = ctc_output\n",
        "\n",
        "        # TF decoders: label strings are contained in sparse tensor\n",
        "        else:\n",
        "            # ctc returns tuple, first element is SparseTensor\n",
        "            decoded = ctc_output[0][0]\n",
        "\n",
        "            # contains string of labels for each batch element\n",
        "            label_strs = [[] for _ in range(batch_size)]\n",
        "\n",
        "            # go over all indices and save mapping: batch -> values\n",
        "            for (idx, idx2d) in enumerate(decoded.indices):\n",
        "                label = decoded.values[idx]\n",
        "                batch_element = idx2d[0]  # index according to [b,t]\n",
        "                label_strs[batch_element].append(label)\n",
        "\n",
        "        # map labels to chars for all batch elements\n",
        "        return [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
        "\n",
        "    def train_batch(self, batch: Batch) -> float:\n",
        "        \"\"\"Feed a batch into the NN to train it.\"\"\"\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "        sparse = self.to_sparse(batch.gt_texts)\n",
        "        eval_list = [self.optimizer, self.loss]\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.gt_texts: sparse,\n",
        "                     self.seq_len: [max_text_len] * num_batch_elements, self.is_train: True}\n",
        "        _, loss_val = self.sess.run(eval_list, feed_dict)\n",
        "        self.batches_trained += 1\n",
        "        return loss_val\n",
        "\n",
        "    @staticmethod\n",
        "    def dump_nn_output(rnn_output: np.ndarray) -> None:\n",
        "        \"\"\"Dump the output of the NN to CSV file(s).\"\"\"\n",
        "        dump_dir = '/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/dump'\n",
        "        if not os.path.isdir(dump_dir):\n",
        "            os.mkdir(dump_dir)\n",
        "\n",
        "        # iterate over all batch elements and create a CSV file for each one\n",
        "        max_t, max_b, max_c = rnn_output.shape\n",
        "        for b in range(max_b):\n",
        "            csv = ''\n",
        "            for t in range(max_t):\n",
        "                for c in range(max_c):\n",
        "                    csv += str(rnn_output[t, b, c]) + ';'\n",
        "                csv += '\\n'\n",
        "            fn = dump_dir + 'rnnOutput_' + str(b) + '.csv'\n",
        "            print('Write dump of NN to file: ' + fn)\n",
        "            with open(fn, 'w') as f:\n",
        "                f.write(csv)\n",
        "\n",
        "    def infer_batch(self, batch: Batch, calc_probability: bool = False, probability_of_gt: bool = False):\n",
        "        \"\"\"Feed a batch into the NN to recognize the texts.\"\"\"\n",
        "\n",
        "        # decode, optionally save RNN output\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "\n",
        "        # put tensors to be evaluated into list\n",
        "        eval_list = []\n",
        "\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            eval_list.append(self.wbs_input)\n",
        "        else:\n",
        "            eval_list.append(self.decoder)\n",
        "\n",
        "        if self.dump or calc_probability:\n",
        "            eval_list.append(self.ctc_in_3d_tbc)\n",
        "\n",
        "        # sequence length depends on input image size (model downsizes width by 4)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "\n",
        "        # dict containing all tensor fed into the model\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.seq_len: [max_text_len] * num_batch_elements,\n",
        "                     self.is_train: False}\n",
        "\n",
        "        # evaluate model\n",
        "        eval_res = self.sess.run(eval_list, feed_dict)\n",
        "\n",
        "        # TF decoders: decoding already done in TF graph\n",
        "        if self.decoder_type != DecoderType.WordBeamSearch:\n",
        "            decoded = eval_res[0]\n",
        "        # word beam search decoder: decoding is done in C++ function compute()\n",
        "        else:\n",
        "            decoded = self.decoder.compute(eval_res[0])\n",
        "\n",
        "        # map labels (numbers) to character string\n",
        "        texts = self.decoder_output_to_text(decoded, num_batch_elements)\n",
        "\n",
        "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
        "        probs = None\n",
        "        if calc_probability:\n",
        "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
        "            ctc_input = eval_res[1]\n",
        "            eval_list = self.loss_per_element\n",
        "            feed_dict = {self.saved_ctc_input: ctc_input, self.gt_texts: sparse,\n",
        "                         self.seq_len: [max_text_len] * num_batch_elements, self.is_train: False}\n",
        "            loss_vals = self.sess.run(eval_list, feed_dict)\n",
        "            probs = np.exp(-loss_vals)\n",
        "\n",
        "        # dump the output of the NN to CSV file(s)\n",
        "        if self.dump:\n",
        "            self.dump_nn_output(eval_res[1])\n",
        "\n",
        "        return texts, probs\n",
        "\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save model to file.\"\"\"\n",
        "        self.snap_ID += 1\n",
        "        self.saver.save(self.sess, '../model/snapshot', global_step=self.snap_ID)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS86TFRvQQsO"
      },
      "source": [
        "#Main Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8t0EZa-HxJS"
      },
      "source": [
        "class FilePaths:\n",
        "    \"\"\"Filenames and paths to data.\"\"\"\n",
        "    fn_char_list = '/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/charList.txt'\n",
        "    fn_summary = '/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/summary.json'\n",
        "    fn_corpus = '/content/drive/MyDrive/Colab Notebooks/simpleHTRLine/data/corpus.txt'\n",
        "\n",
        "decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
        "                       'beamsearch': DecoderType.BeamSearch,\n",
        "                       'wordbeamsearch': DecoderType.WordBeamSearch}\n",
        "decoder_type = decoder_mapping['bestpath']\n",
        "\n",
        "\n",
        "def get_img_height() -> int:\n",
        "    \"\"\"Fixed height for NN.\"\"\"\n",
        "    return 32\n",
        "\n",
        "\n",
        "def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n",
        "    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n",
        "    if line_mode:\n",
        "        return 256, get_img_height()\n",
        "    return 128, get_img_height()\n",
        "\n",
        "\n",
        "def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
        "    \"\"\"Writes training summary file for NN.\"\"\"\n",
        "    with open(FilePaths.fn_summary, 'w') as f:\n",
        "        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n",
        "\n",
        "\n",
        "def train(model: Model,\n",
        "          loader: DataLoaderIAM,\n",
        "          line_mode: bool,\n",
        "          early_stopping: int = 25) -> None:\n",
        "    \"\"\"Trains NN.\"\"\"\n",
        "    epoch = 0  # number of training epochs since start\n",
        "    summary_char_error_rates = []\n",
        "    summary_word_accuracies = []\n",
        "    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n",
        "    best_char_error_rate = float('inf')  # best valdiation character error rate\n",
        "    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n",
        "    # stop training after this number of epochs without improvement\n",
        "    while True:\n",
        "        epoch += 1\n",
        "        print('Epoch:', epoch)\n",
        "\n",
        "        # train\n",
        "        print('Train NN')\n",
        "        loader.train_set()\n",
        "        while loader.has_next():\n",
        "            iter_info = loader.get_iterator_info()\n",
        "            batch = loader.get_next()\n",
        "            batch = preprocessor.process_batch(batch)\n",
        "            loss = model.train_batch(batch)\n",
        "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
        "\n",
        "        # validate\n",
        "        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n",
        "\n",
        "        # write summary\n",
        "        summary_char_error_rates.append(char_error_rate)\n",
        "        summary_word_accuracies.append(word_accuracy)\n",
        "        write_summary(summary_char_error_rates, summary_word_accuracies)\n",
        "\n",
        "        # if best validation accuracy so far, save model parameters\n",
        "        if char_error_rate < best_char_error_rate:\n",
        "            print('Character error rate improved, save model')\n",
        "            best_char_error_rate = char_error_rate\n",
        "            no_improvement_since = 0\n",
        "            model.save()\n",
        "        else:\n",
        "            print(f'Character error rate not improved, best so far: {char_error_rate * 100.0}%')\n",
        "            no_improvement_since += 1\n",
        "\n",
        "        # stop training if no more improvement in the last x epochs\n",
        "        if no_improvement_since >= early_stopping:\n",
        "            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n",
        "            break\n",
        "\n",
        "\n",
        "def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n",
        "    \"\"\"Validates NN.\"\"\"\n",
        "    print('Validate NN')\n",
        "    loader.validation_set()\n",
        "    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n",
        "    num_char_err = 0\n",
        "    num_char_total = 0\n",
        "    num_word_ok = 0\n",
        "    num_word_total = 0\n",
        "    while loader.has_next():\n",
        "        iter_info = loader.get_iterator_info()\n",
        "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
        "        batch = loader.get_next()\n",
        "        batch = preprocessor.process_batch(batch)\n",
        "        recognized, _ = model.infer_batch(batch)\n",
        "\n",
        "        print('Ground truth -> Recognized')\n",
        "        for i in range(len(recognized)):\n",
        "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
        "            num_word_total += 1\n",
        "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
        "            num_char_err += dist\n",
        "            num_char_total += len(batch.gt_texts[i])\n",
        "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
        "                  '\"' + recognized[i] + '\"')\n",
        "\n",
        "    # print validation result\n",
        "    char_error_rate = num_char_err / num_char_total\n",
        "    word_accuracy = num_word_ok / num_word_total\n",
        "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
        "    return char_error_rate, word_accuracy\n",
        "\n",
        "\n",
        "def infer(model: Model, fn_img: str) -> None:\n",
        "    \"\"\"Recognizes text in image provided by file path.\"\"\"\n",
        "    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
        "    assert img is not None\n",
        "\n",
        "    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n",
        "    img = preprocessor.process_img(img)\n",
        "\n",
        "    batch = Batch([img], None, 1)\n",
        "    recognized, probability = model.infer_batch(batch, True)\n",
        "    print(f'Recognized: \"{recognized[0]}\"')\n",
        "    print(f'Probability: {probability[0]}')\n",
        "    return recognized, probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G40G91X7Que2"
      },
      "source": [
        "#Testing The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plgQ43IbLAr8"
      },
      "source": [
        "def pred_image(img_path):\n",
        "  tf.compat.v1.reset_default_graph()\n",
        "  test_preprocess(img_path)\n",
        "  model = Model(list(open(FilePaths.fn_char_list).read()), decoder_type, must_restore=True)\n",
        "  recognized, probability = infer(model, img_path)\n",
        "  return recognized, probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBaDPWYzQ-XG"
      },
      "source": [
        "#API Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VO10TxlS0De",
        "outputId": "d92d60fe-c3f4-4b9a-de93-642ab32ed51b"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8psooiK7MynL"
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import os\n",
        "import requests\n",
        "from string import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "pxMoUeolSwnv",
        "outputId": "acddd00e-d7a2-49a2-df7e-cc5a58150482"
      },
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   \n",
        "\n",
        "def delete_image_after_analysis(img_path):\n",
        "  os.remove('/content/drive/MyDrive/Colab Notebooks/APIimages/' + img_path)\n",
        "\n",
        "def compare_texts(recognized_txt, suggestionList):\n",
        "  output_dict = {}\n",
        "  for i in range(len(suggestionList)):\n",
        "    counter = 0\n",
        "    if len(suggestionList[i]) >= len(recognized_txt):\n",
        "      for ii in range(len(recognized_txt)):\n",
        "        if suggestionList[i][ii].lower() == recognized_txt[ii].lower():\n",
        "           counter += 1\n",
        "      output_dict[suggestionList[i]] = counter \n",
        "    else:\n",
        "      for ii in range(len(suggestionList[i])):\n",
        "        if suggestionList[i][ii].lower() == recognized_txt[ii].lower():\n",
        "           counter += 1\n",
        "      output_dict[suggestionList[i]] = counter\n",
        "  return output_dict\n",
        "\n",
        "def get_suggestions(Recognized_text):\n",
        "  new_Recognized_text = ''\n",
        "  num_of_selected_suggestions = 4\n",
        "  for c in punctuation:\n",
        "    Recognized_text = Recognized_text.replace(c, '')\n",
        "  new_Recognized_text = Recognized_text = Recognized_text.strip()\n",
        "  for chr in new_Recognized_text:\n",
        "    if chr == ' ':\n",
        "      new_Recognized_text = new_Recognized_text.replace(chr,'')\n",
        "  url = f'https://rxnav.nlm.nih.gov/REST/spellingsuggestions.json?name={new_Recognized_text}'\n",
        "  response = requests.get(url)\n",
        "  result =response.json()\n",
        "  if type(result['suggestionGroup']['suggestionList']) == dict:\n",
        "    suggestionList = result['suggestionGroup']['suggestionList']['suggestion']\n",
        "    output_string = ''\n",
        "    myDict = compare_texts(new_Recognized_text, suggestionList)\n",
        "    if len(myDict) >= num_of_selected_suggestions:\n",
        "      for i in range(num_of_selected_suggestions):\n",
        "        MaxDictVal = max(myDict, key=myDict.get)\n",
        "        output_string += \"\\t\" + MaxDictVal + \"\\n\"\n",
        "        del myDict[MaxDictVal]\n",
        "      return f\"Recognized text: {new_Recognized_text}\\nSuggestion List:\\n{output_string}\"\n",
        "    else:\n",
        "      for i in range(len(myDict)):\n",
        "        MaxDictVal = max(myDict, key=myDict.get)\n",
        "        output_string += \"\\t\" + MaxDictVal + \"\\n\"\n",
        "        del myDict[MaxDictVal]\n",
        "      return f\"Recognized text: {new_Recognized_text}\\nSuggestion List:\\n{output_string}\"\n",
        "  else:\n",
        "    return f\"Recognized text: {Recognized_text}\" \n",
        "\n",
        "@app.route(\"/\",methods = ['POST'])\n",
        "def home():\n",
        "  image = request.files['image']\n",
        "  path = image.filename # example.png\n",
        "\n",
        "  image.save(os.path.join('/content/drive/MyDrive/Colab Notebooks/APIimages/', path))\n",
        "\n",
        "  Recognized, Probability = pred_image('/content/drive/MyDrive/Colab Notebooks/APIimages/' + path)\n",
        "\n",
        "  delete_image_after_analysis(path)\n",
        "\n",
        "  return get_suggestions(Recognized[0])\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://f331e9e633db.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  '`tf.layers.batch_normalization` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-6-ac3be3b9d0b4>:85: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:987: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Init with stored values from /content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/snapshot-13\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/snapshot-13\n",
            "Recognized: \",  systonre . I\"\n",
            "Probability: 0.0025889561511576176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [14/Jul/2021 07:54:01] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "Init with stored values from /content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/snapshot-13\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Colab Notebooks/simpleHTRLine/model/snapshot-13\n",
            "Recognized: \"fo Doki plasI\"\n",
            "Probability: 0.000555021979380399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [14/Jul/2021 07:54:58] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ds_OJ8gUODy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}